{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# MiniLM Embedding Builder for AI Tutor\n",
    "\n",
    "This Colab-ready notebook mirrors the ingestion pipeline used in the `ai_tutor` project while keeping the `sentence-transformers/all-MiniLM-L6-v2` embedding model.\n",
    "\n",
    "It guides you through:\n",
    "- installing dependencies required for parsing/chunking,\n",
    "- loading the project settings and overriding the embedding provider,\n",
    "- uploading custom study materials,\n",
    "- parsing, chunking, and embedding them with the same utilities the app uses, and\n",
    "- exporting a JSONL chunk index plus vector store files ready for retrieval.\n",
    "\n",
    "Run each cell sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install runtime dependencies (safe to skip if already available)\n",
    "!pip install -q sentence-transformers pymupdf pandas pyarrow tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Configure project paths and settings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from google.colab import files as colab_files  # type: ignore\n",
    "except ImportError:\n",
    "    colab_files = None\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR\n",
    "for candidate in [NOTEBOOK_DIR, *NOTEBOOK_DIR.parents]:\n",
    "    if (candidate / \"src\" / \"ai_tutor\").exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        break\n",
    "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_ROOT))\n",
    "\n",
    "from ai_tutor.config.loader import load_settings\n",
    "from ai_tutor.config.schema import EmbeddingConfig\n",
    "from ai_tutor.data_models import Chunk, Document, DocumentMetadata\n",
    "from ai_tutor.ingestion.chunker import chunk_document\n",
    "from ai_tutor.ingestion.embeddings import EmbeddingClient\n",
    "from ai_tutor.ingestion.parsers import parse_path\n",
    "from ai_tutor.retrieval.simple_store import SimpleVectorStore\n",
    "from ai_tutor.storage import ChunkJsonlStore\n",
    "\n",
    "UPLOAD_DIR = NOTEBOOK_DIR / \"source_documents\"\n",
    "OUTPUT_DIR = NOTEBOOK_DIR / \"notebook_outputs\"\n",
    "VECTOR_STORE_DIR = OUTPUT_DIR / \"vector_store\"\n",
    "CHUNKS_PATH = OUTPUT_DIR / \"chunks.jsonl\"\n",
    "\n",
    "for directory in [UPLOAD_DIR, OUTPUT_DIR, VECTOR_STORE_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "settings = load_settings()\n",
    "minilm_config = EmbeddingConfig(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    provider=\"sentence-transformers\",\n",
    "    batch_size=settings.embeddings.batch_size,\n",
    "    normalize=True,\n",
    ")\n",
    "settings.embeddings = minilm_config\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Chunk size / overlap: {settings.chunking.chunk_size} / {settings.chunking.chunk_overlap}\")\n",
    "print(f\"Embedding provider: {settings.embeddings.provider}\")\n",
    "print(f\"Embedding model: {settings.embeddings.model}\")\n",
    "print(f\"Upload directory: {UPLOAD_DIR.resolve()}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helpers"
   },
   "outputs": [],
   "source": [
    "# Helpers for parsing documents and creating chunks\n",
    "from typing import List\n",
    "\n",
    "def parse_documents(paths: List[Path]) -> List[Document]:\n",
    "    documents: List[Document] = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            document = parse_path(path)\n",
    "        except Exception as err:\n",
    "            text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            metadata = DocumentMetadata(\n",
    "                doc_id=path.stem,\n",
    "                title=path.stem.replace(\"_\", \" \").title(),\n",
    "                source_path=path,\n",
    "                extra={\"format\": path.suffix.lower() or \"txt\", \"parser\": \"fallback\"},\n",
    "            )\n",
    "            document = Document(metadata=metadata, text=text)\n",
    "            print(f\"Fallback parser used for {path.name}: {err}\")\n",
    "        documents.append(document)\n",
    "    print(f\"Loaded {len(documents)} document(s).\")\n",
    "    return documents\n",
    "\n",
    "def chunk_documents(documents: List[Document]) -> List[Chunk]:\n",
    "    chunks: List[Chunk] = []\n",
    "    for document in documents:\n",
    "        chunks.extend(chunk_document(document, settings.chunking))\n",
    "    print(f\"Created {len(chunks)} chunk(s).\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "# Upload source documents (Colab) or gather from the upload directory\n",
    "source_paths = []\n",
    "if colab_files is not None:\n",
    "    uploaded = colab_files.upload()\n",
    "    for filename, data in uploaded.items():\n",
    "        path = UPLOAD_DIR / filename\n",
    "        path.write_bytes(data)\n",
    "        source_paths.append(path)\n",
    "    print(f\"Saved {len(source_paths)} file(s) to {UPLOAD_DIR}.\")\n",
    "else:\n",
    "    print(\"google.colab not available; expecting files to already exist in the upload directory.\")\n",
    "    source_paths = sorted(UPLOAD_DIR.glob('*'))\n",
    "source_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chunk"
   },
   "outputs": [],
   "source": [
    "# Parse and chunk uploaded documents\n",
    "documents = parse_documents(source_paths)\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents loaded; please upload at least one supported file.\")\n",
    "\n",
    "chunks = chunk_documents(documents)\n",
    "if not chunks:\n",
    "    raise ValueError(\"Chunking produced no data; adjust the source material or chunk settings.\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"chunk_id\": chunk.metadata.chunk_id,\n",
    "            \"doc_id\": chunk.metadata.doc_id,\n",
    "            \"source\": chunk.metadata.source_path.name,\n",
    "            \"page\": chunk.metadata.page,\n",
    "            \"token_count\": chunk.token_count,\n",
    "            \"text_preview\": chunk.text[:120] + (\"...\" if len(chunk.text) > 120 else \"\"),\n",
    "        }\n",
    "        for chunk in chunks[:5]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embed"
   },
   "outputs": [],
   "source": [
    "# Build MiniLM embeddings and persist outputs\n",
    "embedder = EmbeddingClient(settings.embeddings)\n",
    "embeddings = embedder.embed_documents(chunk.text for chunk in chunks)\n",
    "\n",
    "for chunk, embedding in zip(chunks, embeddings):\n",
    "    chunk.embedding = embedding\n",
    "\n",
    "chunk_store = ChunkJsonlStore(CHUNKS_PATH)\n",
    "chunk_store.upsert(chunks)\n",
    "\n",
    "vector_store = SimpleVectorStore(VECTOR_STORE_DIR)\n",
    "vector_store.add(chunks)\n",
    "vector_store.persist()\n",
    "\n",
    "print(f\"Saved chunk index \u2192 {CHUNKS_PATH.resolve()}\")\n",
    "print(f\"Saved vector store \u2192 {VECTOR_STORE_DIR.resolve()}\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"chunk_id\": chunk.metadata.chunk_id,\n",
    "            \"embedding_dim\": len(chunk.embedding) if chunk.embedding is not None else None,\n",
    "        }\n",
    "        for chunk in chunks[:5]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Bundle outputs for optional download\n",
    "import shutil\n",
    "\n",
    "archive_path = shutil.make_archive(OUTPUT_DIR.as_posix(), \"zip\", root_dir=OUTPUT_DIR)\n",
    "print(f\"Created archive: {archive_path}\")\n",
    "if colab_files is not None:\n",
    "    colab_files.download(archive_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MiniLM Embedding Builder for RAG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}