from __future__ import annotations

import asyncio
import logging
import os
import re
from dataclasses import dataclass, field
from datetime import datetime
import json
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

from openai.types.responses import ResponseContentPartDoneEvent, ResponseTextDeltaEvent

from agents import Agent, RawResponsesStreamEvent, Runner, function_tool
from agents import SQLiteSession

from .ingestion import build_ingestion_agent
from .qa import build_qa_agent
from .web import build_web_agent

from ai_tutor.config.schema import RetrievalConfig
from ai_tutor.data_models import RetrievalHit
from ai_tutor.ingestion.embeddings import EmbeddingClient
from ai_tutor.learning.models import LearnerProfile
from ai_tutor.learning.quiz import Quiz, QuizService
from ai_tutor.learning.quiz import Quiz, QuizEvaluation, QuizService
from ai_tutor.retrieval.retriever import Retriever
from ai_tutor.retrieval.vector_store import VectorStore
# SearchTool removed - web_agent uses WebSearchTool directly

logger = logging.getLogger(__name__)


@dataclass
class TutorResponse:
    """
    Structured response container for tutor interactions.
    
    This dataclass encapsulates all information returned from a tutoring session,
    including the generated answer, retrieval evidence, personalization hints,
    and optional quiz payloads.
    
    Attributes
    ----------
    answer : str
        The generated response text from the tutor agent. May include citation
        markers like [1], [2] that correspond to the citations list.
    hits : List[RetrievalHit]
        Raw retrieval results from the vector store, containing chunks and similarity
        scores. Empty for web-sourced or quiz-only responses.
    citations : List[str]
        Formatted citation strings (e.g., "[1] Title (Doc: id, Page: 42)") that
        map to bracketed indices in the answer text. Empty if no local sources used.
    style : str
        The explanation style used for this response. One of "scaffolded" (detailed,
        step-by-step), "stepwise" (moderate guidance), or "concise" (advanced).
    next_topic : Optional[str]
        Suggested next topic for the learner based on their current progress and
        knowledge gaps. None if not applicable.
    difficulty : Optional[str]
        Current difficulty level label for the learner in this domain. One of
        "foundational guidance", "guided practice", or "independent challenge".
    source : Optional[str]
        Origin of the answer. One of "local" (RAG from course materials), "web"
        (internet search), "quiz" (assessment generation), or None.
    quiz : Optional[Quiz]
        Generated quiz object if the interaction produced an assessment. Contains
        questions, correct answers, and explanations.
    """

    answer: str
    hits: List[RetrievalHit]
    citations: List[str]
    style: str
    next_topic: Optional[str] = None
    difficulty: Optional[str] = None
    source: Optional[str] = None
    quiz: Optional[Quiz] = None


@dataclass
class AgentState:
    """
    Shared state object for multi-agent communication.
    
    This mutable state container allows specialist agents (QA, Web, Quiz) to
    store their results, which the orchestrator then collects and formats into
    a unified TutorResponse. The state is reset at the start of each new query.
    
    Attributes
    ----------
    last_hits : List[RetrievalHit]
        Retrieval results from the most recent vector search. Used by QA agent
        to pass evidence back to orchestrator.
    last_citations : List[str]
        Formatted citation strings generated by specialist agents. Collected
        and attached to the final response.
    last_source : Optional[str]
        Identifier for the agent that handled the last query ("local", "web", etc.).
        Helps orchestrator determine response formatting.
    last_quiz : Optional[Quiz]
        Quiz object generated by the quiz tool. If present, the response includes
        interactive assessment UI.
    """
    
    last_hits: List[RetrievalHit] = field(default_factory=list)
    last_citations: List[str] = field(default_factory=list)
    last_source: Optional[str] = None
    last_quiz: Optional[Quiz] = None

    def reset(self) -> None:
        """
        Clear all state variables to prepare for a new query.
        
        Called at the start of each answer() invocation to ensure agents don't
        carry over results from previous interactions.
        """
        self.last_hits.clear()
        self.last_citations.clear()
        self.last_source = None
        self.last_quiz = None


class TutorAgent:
    """
    Multi-agent tutoring orchestrator using the OpenAI Agents SDK.
    
    This class implements a hierarchical agent architecture where an orchestrator
    routes student queries to specialist agents based on query type and content.
    The system supports:
    - STEM Q&A with retrieval-augmented generation (QA Agent)
    - Current events and web searches (Web Agent)
    - Document ingestion and corpus management (Ingestion Agent)
    - Interactive quiz generation and evaluation (Quiz Service)
    
    The orchestrator maintains conversation context through SQLite sessions that
    automatically rotate daily to prevent token overflow. All specialist results
    are collected via a shared AgentState object and formatted into structured
    TutorResponse objects.
    
    Architecture
    ------------
    Student Query → Orchestrator Agent → [QA | Web | Ingestion] Agent → Response
                                       ↘ Quiz Service → Interactive Assessment
    
    Attributes
    ----------
    MIN_CONFIDENCE : float
        Minimum similarity score (0.2) for retrieval results to be considered
        relevant. Queries with no hits above this threshold fall back to web search.
    retriever : Retriever
        Vector search coordinator that embeds queries and retrieves similar chunks.
    # Web search is handled directly by web_agent using WebSearchTool
    ingest_fn : Callable
        Directory ingestion function for processing and indexing new documents.
    sessions : Dict[str, SQLiteSession]
        In-memory cache of active conversation sessions, keyed by learner_id.
    state : AgentState
        Shared mutable state for inter-agent communication.
    session_db_path : Path
        SQLite database path for persistent conversation history.
    quiz_service : QuizService
        Assessment generator that creates and evaluates quizzes.
    """

    MIN_CONFIDENCE = 0.2  # Minimum retrieval score for accepting local results

    def __init__(
        self,
        retrieval_config: RetrievalConfig,
        embedder: EmbeddingClient,
        vector_store: VectorStore,
        ingest_directory: Callable[[Path], object],
        session_db_path: Path,
        quiz_service: QuizService,
        mcp_server: Optional[Any] = None,  # Optional MCP server for Chroma
    ):
        """
        Initialize the multi-agent system with all required dependencies.
        
        Parameters
        ----------
        retrieval_config : RetrievalConfig
            Configuration for vector search (top_k, thresholds, etc.).
        embedder : EmbeddingClient
            Sentence transformer for encoding queries and documents.
        vector_store : VectorStore
            Indexed vector database for similarity search.
        # Web search handled directly by web_agent
        ingest_directory : Callable[[Path], object]
            Function to process and index documents from a directory.
        session_db_path : Path
            SQLite database path for conversation persistence.
        quiz_service : QuizService
            Service for generating and evaluating assessments.
        """
        # Core retrieval infrastructure
        self.retriever = Retriever(retrieval_config, embedder=embedder, vector_store=vector_store)
        self.ingest_fn = ingest_directory
        self.mcp_server = mcp_server  # Optional MCP server for Chroma (persistent across queries)
        
        if mcp_server:
            logger.info("[TutorAgent] MCP server provided - agents will use MCP tools with cached tool list")
        
        # Session and state management
        self.sessions: Dict[str, SQLiteSession] = {}  # Learner ID → Active session
        self.session_turn_counts: Dict[str, int] = {}  # Track turns per session for pruning
        self.state = AgentState()  # Shared state for agent communication
        self.session_db_path = session_db_path
        self.max_turns_per_session = 3  # Prune history: create new session after 3 turns
        
        # Quiz and assessment infrastructure
        self.quiz_service = quiz_service
        
        # Temporary context for quiz generation (set during answer() calls)
        self._active_profile: Optional[LearnerProfile] = None
        self._active_extra_context: Optional[str] = None

        # Agent instances (initialized in _build_agents)
        self.ingestion_agent: Agent | None = None
        self.qa_agent: Agent | None = None
        self.web_agent: Agent | None = None
        self.orchestrator_agent: Agent | None = None

        self._build_agents()

    def _build_agents(self) -> None:
        """
        Construct the multi-agent hierarchy with proper handoff configuration.
        
        This method builds three specialist agents and one orchestrator:
        1. Ingestion Agent: Handles document upload and processing requests
        2. Web Agent: Performs web searches for non-local knowledge
        3. QA Agent: Answers STEM questions using RAG over course materials
        4. Orchestrator: Routes queries to appropriate specialists
        
        The QA Agent is configured with a handoff to the Web Agent, allowing
        automatic fallback when local retrieval yields insufficient results.
        
        Additionally, a generate_quiz tool is defined inline to enable the
        orchestrator to create assessments without agent handoff overhead.
        """
        # Build specialist agents (built once, reused across all queries for persistent context)
        # MCP server connection is shared and tool list is cached to avoid redundant API calls
        logger.debug("[TutorAgent] Building agents with persistent MCP server context")
        self.ingestion_agent = build_ingestion_agent(self.ingest_fn)
        # Web agent uses WebSearchTool directly - no SearchTool wrapper needed
        self.web_agent = build_web_agent(state=self.state)
        self.qa_agent = build_qa_agent(
            self.retriever, 
            self.state, 
            self.MIN_CONFIDENCE, 
            handoffs=[self.web_agent],  # Allow QA → Web fallback
            mcp_server=self.mcp_server,  # Pass persistent MCP server (tools cached per session)
        )
        logger.info("[TutorAgent] Agents built - MCP tools will be cached and reused across queries")

        @function_tool
        def generate_quiz(topic: str, count: int = 4, difficulty: str | None = None) -> str:
            """
            Generate an interactive quiz on a given topic.
            
            Creates a multiple-choice quiz and displays it in an interactive interface
            where students can select answers and get immediate feedback.
            
            Parameters
            ----------
            topic : str
                Subject matter for the quiz (e.g., "machine learning", "physics", "calculus").
                Should be a broad, searchable topic, NOT "documents" or "uploaded files".
            count : int, default=4
                Number of questions to generate. Valid range: 3 to 40.
                IMPORTANT: If user says "create 20 quizzes", use count=20 (their exact number).
                Only use default (4) if user doesn't specify a number.
            difficulty : str | None, optional
                Explicit difficulty level. If None, inferred from learner profile.
            
            Returns
            -------
            str
                Confirmation message for the orchestrator to relay to the student.
                
            Examples
            --------
            User says: "create 20 quizzes from documents"
            → Call: generate_quiz(topic='computer science', count=20)
            
            User says: "quiz me on calculus"  
            → Call: generate_quiz(topic='calculus', count=4)
            """
            # Validate and clamp question count to reasonable range
            try:
                question_count = int(count)
            except (TypeError, ValueError):
                question_count = 4
            question_count = max(3, min(question_count, 40))  # Enforce [3-40] range
            
            # Access the currently active learner profile (set by answer() method)
            profile = self._active_profile
            
            # Generate quiz using the quiz service with all available context
            quiz = self.quiz_service.generate_quiz(
                topic=topic,
                profile=profile,
                num_questions=question_count,
                difficulty=difficulty,
                extra_context=self._active_extra_context,  # Include uploaded docs
            )
            
            # Store in shared state for orchestrator collection
            self.state.last_quiz = quiz
            self.state.last_source = "quiz"
            
            # Return message for orchestrator to communicate quiz readiness
            return (
                f"Prepared a {len(quiz.questions)}-question quiz on {quiz.topic}. "
                "Let the learner know the quiz is ready for them to take."
            )

        handoffs = [agent for agent in (self.ingestion_agent, self.qa_agent, self.web_agent) if agent is not None]
        self.orchestrator_agent = Agent(
            name="tutor_orchestrator",
            model="gpt-4o-mini",
            instructions=(
                "You are the Orchestrator Agent in a multi-agent tutoring system. Your PRIMARY job is routing queries to specialist agents and tools. "
                "⚠️ ALWAYS check conversation history before routing. If a complete answer exists, return it; do not re-route. "
                "Each question is routed ONCE per agent.\n\n"
                
                "ROUTING RULES:\n"
                "- STEM questions (math, physics, engineering, CS, technical concepts) → qa_agent. Do NOT answer yourself.\n"
                "- Current events, news, history, literature, arts → web_agent. Route only once.\n"
                "- File uploads / ingestion → ingestion_agent.\n"
                "- Quiz requests → ALWAYS call generate_quiz(topic, count) tool. Do NOT generate questions in text.\n\n"

                "TOOL USAGE:\n"
                "- generate_quiz has automatic access to uploaded documents.\n"
                "- Extract topic and count from user query; default count=4 if unspecified.\n"
                "- Never refuse quiz requests.\n\n"

                "CONVERSATION HANDLING:\n"
                "- Prevent loops: never route same agent twice for the same question.\n"
                "- Cache domain classification per thread to avoid redundant LLM calls.\n"
                "- Use previous agent responses to guide routing.\n"
                "- Stop routing when an answer exists.\n\n"

                "PRIORITY:\n"
                "- Routing and tool calls ONLY, no direct answering (except greetings, system questions, or general knwoledge)."
            ),
            tools=[generate_quiz],
            handoffs=handoffs,
        )


    def evaluate_quiz(
        self,
        *,
        quiz: Quiz,
        answers: List[int],
        profile: Optional[LearnerProfile],
    ) -> QuizEvaluation:
        """Score a learner's quiz submission and return detailed feedback."""
        return self.quiz_service.evaluate_quiz(
            quiz=quiz,
            answers=answers,
            profile=profile,
        )

    def answer(
        self,
        learner_id: str,
        question: str,
        mode: str,
        style_hint: str,
        profile: Optional[LearnerProfile] = None,
        extra_context: Optional[str] = None,
        on_delta: Optional[Callable[[str], None]] = None,
    ) -> TutorResponse:
        """Synchronously orchestrate the multi-agent run and produce a TutorResponse."""
        try:
            # Check if we're already in an event loop
            asyncio.get_running_loop()
            # If we're in a loop, raise an error suggesting to use answer_async instead
            raise RuntimeError(
                "Cannot call answer() from within an async context. "
                "Use answer_async() instead or call from a sync context."
            )
        except RuntimeError as e:
            if "Cannot call answer()" in str(e):
                raise
            # No running loop, safe to use asyncio.run()
            return asyncio.run(
                self._answer_async(
                    learner_id=learner_id,
                    question=question,
                    mode=mode,
                    style_hint=style_hint,
                    profile=profile,
                    extra_context=extra_context,
                    on_delta=on_delta,
                )
            )
    
    async def answer_async(
        self,
        learner_id: str,
        question: str,
        mode: str,
        style_hint: str,
        profile: Optional[LearnerProfile] = None,
        extra_context: Optional[str] = None,
        on_delta: Optional[Callable[[str], None]] = None,
    ) -> TutorResponse:
        """Asynchronously orchestrate the multi-agent run and produce a TutorResponse."""
        return await self._answer_async(
            learner_id=learner_id,
            question=question,
            mode=mode,
            style_hint=style_hint,
            profile=profile,
            extra_context=extra_context,
            on_delta=on_delta,
        )

    async def _answer_async(
        self,
        learner_id: str,
        question: str,
        mode: str,
        style_hint: str,
        profile: Optional[LearnerProfile],
        extra_context: Optional[str],
        on_delta: Optional[Callable[[str], None]],
    ) -> TutorResponse:
        import time
        answer_start = time.time()
        logger.info(f"[TutorAgent] Starting answer generation for question: {question[:100]}...")
        
        # Check if we need to prune history before getting session
        # This ensures we create a new session when we're about to exceed the turn limit
        current_turn = self.session_turn_counts.get(learner_id, 0)
        if current_turn > 0 and current_turn % self.max_turns_per_session == 0:
            # We've reached the turn limit, create a new session (prune history)
            logger.info(f"[TutorAgent] Pruning history: creating new session after {current_turn} turns (keeping only last {self.max_turns_per_session} turns)")
            if learner_id in self.sessions:
                del self.sessions[learner_id]
        
        session = self._get_session(learner_id)
        self.state.reset()
        
        # Increment turn count for history pruning
        self.session_turn_counts[learner_id] = current_turn + 1

        # For orchestrator: minimal prompt with just the question
        # The specialist agents will get the full context with style hints
        if self.orchestrator_agent:
            prompt_sections: List[str] = []
            if profile:
                prompt_sections.append("Learner profile summary:")
                prompt_sections.append(self._render_profile_summary(profile))
                prompt_sections.append("")
            prompt_sections.append("Question:")
            prompt_sections.append(question)
            prompt = "\n".join(prompt_sections)
        else:
            # Fallback for when orchestrator is not used
            system_preamble = (
                f"Learner mode: {mode}. Preferred explanation style: {style_hint}. "
                "Cite supporting evidence using bracketed indices or URLs when available."
            )
            prompt_sections: List[str] = [system_preamble, ""]

            if profile:
                prompt_sections.append("Learner profile summary:")
                prompt_sections.append(self._render_profile_summary(profile))
                prompt_sections.append("")

            if extra_context:
                prompt_sections.append("Session documents:")
                prompt_sections.append(extra_context)
                prompt_sections.append("")

            prompt_sections.append("Question:")
            prompt_sections.append(question)
            prompt = "\n".join(prompt_sections)

        self._active_profile = profile
        self._active_extra_context = extra_context
        self.state.last_quiz = None
        try:
            raw_answer = await self._run_specialist(
                prompt,
                session,
                on_delta,
            )
        finally:
            self._active_profile = None
            self._active_extra_context = None
        quiz_payload: Optional[Quiz] = None
        answer_text = raw_answer
        if raw_answer.strip().startswith("{"):
            processed, computed_quiz = self._process_quiz_directive(
                raw_answer,
                profile=profile,
                extra_context=extra_context,
            )
            if computed_quiz is not None:
                quiz_payload = computed_quiz
                answer_text = processed
        if quiz_payload is None and self.state.last_quiz is not None:
            quiz_payload = self.state.last_quiz
        if quiz_payload is None and self._should_force_quiz(question):
            quiz_payload = self.quiz_service.generate_quiz(
                topic=self._infer_topic_from_request(question),
                profile=profile,
                num_questions=self._infer_count_from_request(question),
                difficulty=None,
                extra_context=extra_context,
            )
            self.state.last_quiz = quiz_payload
            answer_text = (
                f"I've prepared a {len(quiz_payload.questions)}-question quiz on {quiz_payload.topic} for you. "
                "You can start taking the quiz now!"
            )
        if not quiz_payload and not self.state.last_citations:
            answer_text = self._strip_citation_markers(answer_text)
        if quiz_payload is None:
            processed, computed_quiz = self._process_quiz_directive(
                answer_text,
                profile=profile,
                extra_context=extra_context,
            )
            if computed_quiz is not None:
                quiz_payload = computed_quiz
                answer_text = processed
        hits = self.state.last_hits if not quiz_payload else []
        citations = self.state.last_citations if not quiz_payload else []
        source = "quiz" if quiz_payload else self.state.last_source

        total_answer_duration = time.time() - answer_start
        logger.info(f"[TutorAgent] Answer generation complete in {total_answer_duration:.2f}s (answer length: {len(answer_text)} chars)")

        return TutorResponse(
            answer=answer_text,
            hits=hits,
            citations=citations,
            style=style_hint,
            source=source,
            quiz=quiz_payload,
        )

    def _get_session(self, learner_id: str) -> SQLiteSession:
        """
        Get or create a conversation session with automatic daily rotation and history pruning.
        
        This method implements token overflow prevention through:
        1. Date-based session keys (daily rotation)
        2. Turn-based pruning (new session after max_turns_per_session turns)
        
        This prevents prompt length from growing unbounded while maintaining recent context
        for anti-loop detection and conversation coherence.
        
        Session Key Format
        ------------------
        ai_tutor_{learner_id}_{YYYYMMDD}_{turn_batch}
        
        Example: "ai_tutor_student123_20251023_0" (first 3 turns)
                 "ai_tutor_student123_20251023_1" (next 3 turns)
        
        Parameters
        ----------
        learner_id : str
            Unique identifier for the learner (used for profile lookup and session key).
        
        Returns
        -------
        SQLiteSession
            Active session object for this learner. History is pruned to last 3 turns
            to prevent prompt poisoning and reduce synthesis time.
        
        Notes
        -----
        - Sessions are cached in memory for performance
        - Automatic rotation occurs at midnight (based on server timezone)
        - History pruning: new session created after max_turns_per_session turns
        - Previous sessions remain in SQLite but are not loaded into context
        - Manual clearing available via clear_session() or clear_sessions.py script
        """
        # Use date-based session IDs to auto-rotate daily (prevents token accumulation)
        today = datetime.now().strftime("%Y%m%d")
        
        # Check current turn count for this learner
        turn_count = self.session_turn_counts.get(learner_id, 0)
        turn_batch = turn_count // self.max_turns_per_session
        
        # Create session key with turn batch to enable pruning
        session_key = f"ai_tutor_{learner_id}_{today}_{turn_batch}"
        
        # Check if we have a cached session for this learner
        session = self.sessions.get(learner_id)
        
        # Create new session if:
        # 1. None exists
        # 2. Session key changed (new day or new turn batch)
        if (session is None or 
            getattr(session, '_session_key', None) != session_key):
            
            session = SQLiteSession(
                session_key,
                db_path=str(self.session_db_path),
            )
            # Store session key for comparison on next access
            session._session_key = session_key  # type: ignore
            self.sessions[learner_id] = session
            logger.debug(f"Created new session for {learner_id}: {session_key} (turn batch {turn_batch})")
        
        return session
    
    def clear_session(self, learner_id: str) -> None:
        """
        Clear the in-memory conversation session for a learner.
        
        This removes the cached session from memory, forcing creation of a fresh
        session on the next interaction. The SQLite database retains historical
        records, but they won't be loaded into the agent's context.
        
        Parameters
        ----------
        learner_id : str
            Unique identifier for the learner whose session should be cleared.
        
        Notes
        -----
        - Only removes from memory cache, not from SQLite database
        - To permanently delete from database, use scripts/clear_sessions.py
        - Next interaction will create a fresh session with no conversation history
        """
        if learner_id in self.sessions:
            del self.sessions[learner_id]
        if learner_id in self.session_turn_counts:
            del self.session_turn_counts[learner_id]
        # Note: This doesn't delete from SQLite, just removes from memory
        # The session will start fresh on next question

    @staticmethod
    def _strip_citation_markers(answer: str) -> str:
        cleaned = re.sub(r"\[\s*\d+(?:\s*,\s*\d+)*\s*\]", "", answer)
        return re.sub(r"\s{2,}", " ", cleaned).strip()

    @staticmethod
    def _render_profile_summary(profile: LearnerProfile) -> str:
        lines = [
            f"Name: {profile.name or profile.learner_id}",
            f"Total study time (minutes): {profile.total_time_minutes:.1f}",
        ]
        if profile.domain_strengths:
            strengths = sorted(profile.domain_strengths.items(), key=lambda item: item[1], reverse=True)[:3]
            strength_lines = ", ".join(f"{domain}: {score:.2f}" for domain, score in strengths)
            lines.append(f"Domain strengths: {strength_lines}")
        if profile.difficulty_preferences:
            prefs = ", ".join(f"{domain}: {pref}" for domain, pref in list(profile.difficulty_preferences.items())[:3])
            lines.append(f"Difficulty preferences: {prefs}")
        if profile.next_topics:
            next_topics = ", ".join(f"{domain}: {topic}" for domain, topic in list(profile.next_topics.items())[:3])
            lines.append(f"Upcoming topics: {next_topics}")
        return "\n".join(lines)

    def _process_quiz_directive(
        self,
        answer_text: str,
        profile: Optional[LearnerProfile],
        extra_context: Optional[str],
    ) -> tuple[str, Optional[Quiz]]:
        text = answer_text.strip()
        if not text.startswith("{"):
            return answer_text, None
        try:
            payload = json.loads(text)
        except json.JSONDecodeError:
            return answer_text, None
        if not isinstance(payload, dict):
            return answer_text, None
        if payload.get("action") != "generate_quiz":
            return answer_text, None

        topic_raw = payload.get("topic") or payload.get("subject") or "general review"
        topic = str(topic_raw).strip() or "general review"
        count_raw = payload.get("count") or payload.get("num_questions") or 4
        try:
            count = int(count_raw)
        except (TypeError, ValueError):
            count = 4
        count = max(3, min(count, 40))  # Allow up to 40 questions
        message = str(payload.get("message") or "").strip()

        quiz = self.quiz_service.generate_quiz(
            topic=topic,
            profile=profile,
            num_questions=count,
            difficulty=None,
            extra_context=extra_context,
        )
        if not message:
            message = f"I've prepared a {count}-question quiz on {quiz.topic}. Scroll down to take it."
        return message, quiz

    @staticmethod
    def _should_force_quiz(question: str) -> bool:
        lowered = question.lower()
        keywords = [
            "quiz",
            "quizzes",
            "practice questions",
            "practice quiz",
            "give me questions",
            "mcq",
            "multiple choice",
            "test me",
        ]
        return any(keyword in lowered for keyword in keywords)

    @staticmethod
    def _infer_topic_from_request(question: str) -> str:
        # Check if requesting quiz from documents
        if re.search(r"from\s+(?:the\s+)?documents?|based\s+on\s+documents?|using\s+(?:the\s+)?(?:uploaded\s+)?(?:documents?|files?)", question, flags=re.IGNORECASE):
            return "uploaded documents"
        
        match = re.search(r"(?:about|on|regarding)\s+(.+)", question, flags=re.IGNORECASE)
        topic = match.group(1).strip() if match else question.strip()
        topic = re.sub(r"[\.\?!]+$", "", topic)
        # Clean up quiz-related words from topic
        topic = re.sub(r"\b(?:quiz|quizzes|questions?|test)\b", "", topic, flags=re.IGNORECASE).strip()
        return topic or "general review"

    @staticmethod
    def _infer_count_from_request(question: str) -> int:
        match = re.search(r"(\d+)\s*(?:question|questions|quiz|quizzes|mcq)", question, flags=re.IGNORECASE)
        if not match:
            match = re.search(r"(\d+)", question)
        if match:
            try:
                value = int(match.group(1))
                return max(3, min(value, 40))  # Maximum number of questions is 40
            except ValueError:
                pass
        return 4

    async def _run_specialist(
        self,
        prompt: str,
        session: SQLiteSession,
        on_delta: Optional[Callable[[str], None]],
    ) -> str:
        import time
        start_time = time.time()
        final_tokens: List[str] = []
        max_iterations = 3  # Prevent infinite routing loops
        iteration = 0

        agent_to_run = self.orchestrator_agent or self.qa_agent
        
        try:
            while iteration < max_iterations:
                iteration += 1
                iteration_start = time.time()
                logger.info(f"[TutorAgent] Running agent iteration {iteration}/{max_iterations}")
                
                try:
                    # Use non-streaming for more reliable output capture
                    # Streaming can miss final output in some cases
                    # Add timeout to prevent hanging on MCP server calls
                    logger.debug(f"[TutorAgent] Running agent (non-streaming) for iteration {iteration}")
                    import asyncio
                    try:
                        result = await asyncio.wait_for(
                            Runner.run(agent_to_run, input=prompt, session=session),
                            timeout=60.0  # 60 second timeout to prevent infinite hangs
                        )
                    except asyncio.TimeoutError:
                        logger.error(f"[TutorAgent] Agent run timed out after 60 seconds in iteration {iteration}")
                        raise RuntimeError("Agent execution timed out - MCP server may be unresponsive")
                    
                    iteration_duration = time.time() - iteration_start
                    logger.info(f"[TutorAgent] Iteration {iteration} completed in {iteration_duration:.2f}s")
                    
                    if result and result.final_output:
                        response_text = result.final_output.strip()
                        iteration_tokens = [response_text]
                        final_tokens = [response_text]
                        
                        # Stream the output via callback if provided
                        if on_delta:
                            for char in response_text:
                                on_delta(char)
                        
                        logger.info(f"[TutorAgent] Iteration {iteration} response length: {len(response_text)} chars (took {iteration_duration:.2f}s)")
                        logger.debug(f"[TutorAgent] Response preview: {response_text[:200]}...")
                    else:
                        logger.warning(f"[TutorAgent] Iteration {iteration} returned no final_output (took {iteration_duration:.2f}s)")
                        response_text = ""
                        iteration_tokens = []
                    
                    # If we got a substantial answer (more than just routing instructions), return it
                    if len(response_text) > 50 and not any(keyword in response_text.lower() for keyword in [
                        "transfer_to_qa_agent", "transfer_to_web_agent", "handing off", "routing to"
                    ]):
                        total_duration = time.time() - start_time
                        logger.info(f"[TutorAgent] Got substantial answer after {iteration} iteration(s) in {total_duration:.2f}s total, returning")
                        break
                    
                    # If we got some response but it's short, check if it's a complete answer
                    if len(response_text) > 20 and len(response_text) <= 50:
                        # Might be a short but valid answer
                        logger.info(f"[TutorAgent] Got short but potentially valid answer: {response_text[:100]}...")
                        # Don't break, continue to see if we get more
                    
                    # If we've exhausted iterations, return what we have
                    if iteration >= max_iterations:
                        logger.warning(f"[TutorAgent] Reached max iterations ({max_iterations}), returning current response")
                        break
                        
                except Exception as e:
                    logger.error(f"[TutorAgent] Error in iteration {iteration}: {e}", exc_info=True)
                    if iteration >= max_iterations:
                        break
                    # Continue to next iteration
                    continue

            # Fallback: if orchestrator didn't work and no local source, try web agent
            if self.orchestrator_agent is None and not self.state.last_source and self.web_agent is not None:
                logger.info("[TutorAgent] No local source found, trying web agent")
                if on_delta:
                    on_delta("[info] No local evidence, searching the web...\n")
                self.state.reset()
                final_tokens.clear()
                try:
                    web_result = await Runner.run(self.web_agent, input=prompt, session=session)
                    if web_result and web_result.final_output:
                        web_text = web_result.final_output.strip()
                        final_tokens.append(web_text)
                        if on_delta:
                            for char in web_text:
                                on_delta(char)
                        logger.info(f"[TutorAgent] Web agent returned {len(web_text)} chars")
                    else:
                        logger.warning("[TutorAgent] Web agent returned no output")
                except Exception as e:
                    logger.error(f"[TutorAgent] Error in web agent fallback: {e}", exc_info=True)

        except Exception as e:
            logger.error(f"[TutorAgent] Critical error in _run_specialist: {e}", exc_info=True)
            # Return error message instead of empty string
            final_tokens.append(f"I encountered an error while processing your question: {str(e)}. Please try again.")

        result = "".join(final_tokens).strip()
        total_duration = time.time() - start_time
        
        # If result is empty or just routing instructions, try non-streaming fallback
        if not result or len(result) < 20:
            logger.warning(f"[TutorAgent] Empty or very short response ({len(result)} chars) after {total_duration:.2f}s, trying non-streaming fallback")
            try:
                # Try non-streaming run as fallback with timeout
                import asyncio
                fallback_start = time.time()
                fallback_result = await asyncio.wait_for(
                    Runner.run(agent_to_run, input=prompt, session=session),
                    timeout=30.0  # Shorter timeout for fallback
                )
                fallback_duration = time.time() - fallback_start
                if fallback_result and fallback_result.final_output:
                    result = fallback_result.final_output.strip()
                    logger.info(f"[TutorAgent] Got answer from non-streaming fallback: {len(result)} chars (took {fallback_duration:.2f}s)")
            except asyncio.TimeoutError:
                logger.error("[TutorAgent] Non-streaming fallback timed out after 30 seconds")
            except Exception as e:
                logger.error(f"[TutorAgent] Non-streaming fallback also failed: {e}")
        
        # If still empty, provide a helpful fallback message
        if not result or len(result) < 20:
            logger.warning(f"[TutorAgent] Still empty after fallback ({len(result)} chars) after {total_duration:.2f}s total, providing default message")
            if self.state.last_source:
                result = f"I found some information but couldn't generate a complete answer. Please try rephrasing your question."
            elif self.state.last_hits:
                result = "I found relevant materials but had trouble generating a complete answer. Please try rephrasing your question."
            else:
                result = "I apologize, but I'm having trouble generating a complete answer. Please try rephrasing your question or check if the MCP server is running properly."
        
        final_duration = time.time() - start_time
        logger.info(f"[TutorAgent] Final answer length: {len(result)} chars, total time: {final_duration:.2f}s")
        return result
