@startuml
title Question Answering Flow (Multi-Agent Streaming)

actor User
participant CLI as "CLI / Agent Runner"
participant TutorSystem
participant TutorAgent
participant Retriever
participant VectorStore
participant EmbeddingClient
participant SearchTool
participant OpenAI as "OpenAI Agents Runtime"

User -> CLI : Run `ai-tutor ask`\n(learner_id, question, mode)
CLI -> TutorSystem : answer_question(..., stream=True)
TutorSystem -> TutorAgent : stream_answer(learner_id, question, style)

loop Runner.run_streamed
  TutorAgent -> OpenAI : triage_agent receives turn
  alt Ingestion request
    OpenAI -> TutorAgent : handoff to ingestion_agent
    TutorAgent -> TutorSystem : ingest_corpus(directory)
    TutorSystem --> TutorAgent : ingestion summary
  else Local corpus sufficient
    TutorAgent -> Retriever : retrieve(Query)
    Retriever -> EmbeddingClient : embed query
    EmbeddingClient --> Retriever : query embedding
    Retriever -> VectorStore : search top_k
    VectorStore --> Retriever : ranked hits
    Retriever --> TutorAgent : RetrievalHit list
    TutorAgent --> OpenAI : provide context + citations
  else Needs external evidence
    TutorAgent -> SearchTool : web_search(query)
    SearchTool --> TutorAgent : ranked web snippets
    TutorAgent --> OpenAI : supply URLs + snippets
  end
  OpenAI --> TutorAgent : streamed deltas
  TutorAgent --> TutorSystem : delta text
  TutorSystem --> CLI : print delta
end

TutorAgent -> TutorSystem : TutorResponse(answer, hits, citations)
TutorSystem -> CLI : finalize + update personalization
CLI -> User : Render answer + citations

note left of VectorStore
  Cosine similarity search over\nembedded chunks persisted on disk.
end note

note right of TutorAgent
  Maintains learner session memory, routes between\nspecialist agents, and enforces local-first citations.
end note

@enduml
