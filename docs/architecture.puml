@startuml
skinparam componentStyle rectangle
skinparam wrapWidth 200
skinparam maxMessageSize 200
title Personal STEM Instructor â€“ Architecture

actor User

package "Entry Points" {
  component "CLI (`ai-tutor`)" as CLI
  component "OpenAI Agents (demo / custom)" as AgentRunner
  component "Python API / Notebooks" as PythonAPI
}

node "TutorSystem" as TutorSystem {
  component "Settings Loader\n& Logging" as Settings
  component "IngestionPipeline" as Ingestion
  component "TutorAgent" as TutorAgent
  component "Retriever" as Retriever
  component "EmbeddingClient" as EmbeddingClient
  component "SearchTool" as SearchTool
  component "VectorStore\n(SimpleVectorStore)" as VectorStore
  component "ChunkJsonlStore" as ChunkStore
  component "Parsers" as Parsers
  component "Chunker" as Chunker
  component "QA Agent" as QA
  component "Web Agent" as Web
}

database "Vector Store Files\n(data/vector_store)" as VectorFS
database "Chunks Index\n(data/processed/chunks.jsonl)" as ChunkFS
folder "Raw Documents\n(data/raw)" as RawDocs
cloud "OpenAI Agents Runtime" as OpenAIAgents
cloud "Embedding Provider\n(sentence-transformers / optional API)" as EmbeddingProvider

User --> CLI
User --> AgentRunner
User --> PythonAPI

CLI --> Ingestion : `ingest` command
CLI --> TutorAgent : `ask` command
AgentRunner --> Ingestion : ingest_corpus tool
AgentRunner --> TutorAgent : answer_question tool
PythonAPI --> Ingestion : direct ingestion call
PythonAPI --> TutorAgent : answer_question

Ingestion --> Parsers : discover & parse files
Parsers --> RawDocs : read supported documents
Ingestion --> Chunker : create overlapping chunks
Chunker --> Ingestion : chunk metadata + text
Chunker --> EmbeddingClient : request embeddings
EmbeddingClient --> EmbeddingProvider : fetch vectors
EmbeddingClient --> Ingestion : chunk embeddings
Ingestion --> VectorStore : add embeddings
VectorStore --> VectorFS : persist embeddings.npy + metadata.json
Ingestion --> ChunkStore : upsert chunk JSONL
ChunkStore --> ChunkFS : persist chunks.jsonl

TutorAgent --> QA : local evidence (if available)
QA --> Retriever : retrieve local evidence
Retriever --> EmbeddingClient : embed query
Retriever --> VectorStore : cosine search
TutorAgent --> Web : fallback when corpus empty
Web --> SearchTool : hosted web search
TutorAgent --> OpenAIAgents : stream responses

note right of Ingestion
  1. Parse PDFs/Markdown/TXT via Parsers
  2. Chunk content with Chunker
  3. Embed and persist to VectorStore + ChunkStore
end note

note right of TutorAgent
  1. Attempt local retrieval (Retriever + VectorStore).
  2. If confident hits exist, QA agent generates cited answer.
  3. Otherwise Web agent queries hosted search for reputable snippets.
  4. TutorAgent streams the result and personalization metadata back to callers.
end note

@enduml
